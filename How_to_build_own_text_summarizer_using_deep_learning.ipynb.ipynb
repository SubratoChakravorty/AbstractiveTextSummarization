{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFuL-RBgXqgU"
   },
   "source": [
    "In this notebook, we will build an abstractive based text summarizer using deep learning from the scratch in python using keras\n",
    "\n",
    "I recommend you to go through the article over [here](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/) to cover all the concepts which is required to build our own summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5dSoP8lGMZi"
   },
   "source": [
    "Understanding the Problem Statement\n",
    "\n",
    "Customer reviews can often be long and descriptive. Analyzing these reviews manually, as you can imagine, is really time-consuming. This is where the brilliance of Natural Language Processing can be applied to generate a summary for long reviews.\n",
    "\n",
    "We will be working on a really cool dataset. Our objective here is to generate a summary for the Amazon Fine Food reviews using the abstraction-based approach we learned about above. You can download the dataset from[ here ](https://www.kaggle.com/snap/amazon-fine-food-reviews)\n",
    "\n",
    "It’s time to fire up our Jupyter notebooks! Let’s dive into the implementation details right away.\n",
    "\n",
    "#Custom Attention Layer\n",
    "\n",
    "Keras does not officially support attention layer. So, we can either implement our own attention layer or use a third-party implementation. We will go with the latter option for this article. You can download the attention layer from [here](https://github.com/thushv89/attention_keras/blob/master/layers/attention.py) and copy it in a different file called attention.py.\n",
    "\n",
    "Let’s import it into our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi64aA0FFxcS"
   },
   "outputs": [],
   "source": [
    "from attention import AttentionLayer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUValOzcHtEK"
   },
   "source": [
    "#Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "_Jpu8qLEFxcY",
    "outputId": "95968e01-faac-4911-c802-9c008a4e62cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVakjZ3oICgx"
   },
   "source": [
    "#Read the dataset\n",
    "\n",
    "This dataset consists of reviews of fine foods from Amazon. The data spans a period of more than 10 years, including all ~500,000 reviews up to October 2012. These reviews include product and user information, ratings, plain text review, and summary. It also includes reviews from all other Amazon categories.\n",
    "\n",
    "We’ll take a sample of 100,000 reviews to reduce the training time of our model. Feel free to use the entire dataset for training your model if your machine has that kind of computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnK5o4Z1Fxcj"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./amazon-fine-food-reviews/Reviews.csv\",nrows=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGNQKvCaISIn"
   },
   "source": [
    "# Drop Duplicates and NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cjul88oOFxcr"
   },
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text'],inplace=True)#dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)#dropping na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qi0xD6BkIWAm"
   },
   "source": [
    "# Information about dataset\n",
    "\n",
    "Let us look at datatypes and shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__fy-JxTFxc9",
    "outputId": "d42c6e36-bbc8-43c2-de0e-d3effe3e8c4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 997 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      "Id                        997 non-null int64\n",
      "ProductId                 997 non-null object\n",
      "UserId                    997 non-null object\n",
      "ProfileName               997 non-null object\n",
      "HelpfulnessNumerator      997 non-null int64\n",
      "HelpfulnessDenominator    997 non-null int64\n",
      "Score                     997 non-null int64\n",
      "Time                      997 non-null int64\n",
      "Summary                   997 non-null object\n",
      "Text                      997 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 85.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0xLYACiFxdJ"
   },
   "source": [
    "#Preprocessing\n",
    "\n",
    "Performing basic preprocessing steps is very important before we get to the model building part. Using messy and uncleaned text data is a potentially disastrous move. So in this step, we will drop all the unwanted symbols, characters, etc. from the text that do not affect the objective of our problem.\n",
    "\n",
    "Here is the dictionary that we will use for expanding the contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0s6IY-x2FxdL"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JFRXFHmI7Mj"
   },
   "source": [
    "We will perform the below preprocessing tasks for our data:\n",
    "\n",
    "1.Convert everything to lowercase\n",
    "\n",
    "2.Remove HTML tags\n",
    "\n",
    "3.Contraction mapping\n",
    "\n",
    "4.Remove (‘s)\n",
    "\n",
    "5.Remove any text inside the parenthesis ( )\n",
    "\n",
    "6.Eliminate punctuations and special characters\n",
    "\n",
    "7.Remove stopwords\n",
    "\n",
    "8.Remove short words\n",
    "\n",
    "Let’s define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZr-u3OEFxdT"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    if(num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                                 #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2QAeCHWFxdY"
   },
   "outputs": [],
   "source": [
    "#call the function\n",
    "cleaned_text = []\n",
    "for t in data['Text']:\n",
    "    cleaned_text.append(text_cleaner(t,0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snRZY8wjLao2"
   },
   "source": [
    "Let us look at the first five preprocessed reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCAIkhWbFxdh",
    "outputId": "c2da1a36-4488-4e32-ef9e-fcfe496e374d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
       " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\n",
       " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
       " 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\n",
       " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsRXocxoFxd-"
   },
   "outputs": [],
   "source": [
    "#call the function\n",
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(text_cleaner(t,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZeD0gs6Lnb-"
   },
   "source": [
    "Let us look at the first 10 preprocessed summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQJdZcAzFxee",
    "outputId": "a1fbe683-c03f-4afb-addf-e075021c121b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good quality dog food',\n",
       " 'not as advertised',\n",
       " 'delight says it all',\n",
       " 'cough medicine',\n",
       " 'great taffy',\n",
       " 'nice taffy',\n",
       " 'great just as good as the expensive brands',\n",
       " 'wonderful tasty taffy',\n",
       " 'yay barley',\n",
       " 'healthy dog food']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_summary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1zLpnqsFxey"
   },
   "outputs": [],
   "source": [
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KT_D2cLiLy77"
   },
   "source": [
    "#Drop empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYK390unFxfA"
   },
   "outputs": [],
   "source": [
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vm8Fk2TCL7Sp"
   },
   "source": [
    "#Understanding the distribution of the sequences\n",
    "\n",
    "Here, we will analyze the length of the reviews and the summary to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdF76AHHFxgw",
    "outputId": "e3bbe165-4235-482f-bfd4-36a3f1d95290"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwdSGIhGMEbz"
   },
   "source": [
    "Interesting. We can fix the maximum length of the summary to 8 since that seems to be the majority summary length.\n",
    "\n",
    "Let us understand the proportion of the length of summaries below 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JRjwdIOFxg3",
    "outputId": "f968be82-c539-471d-ce23-16f18b059ea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9437185929648241\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in data['cleaned_summary']:\n",
    "    if(len(i.split())<=8):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_summary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYB4Ga9KMjEu"
   },
   "source": [
    "We observe that 94% of the summaries have length below 8. So, we can fix maximum length of summary to 8.\n",
    "\n",
    "Let us fix the maximum length of review to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKD5VOWqFxhC"
   },
   "outputs": [],
   "source": [
    "max_text_len=30\n",
    "max_summary_len=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6d48E-8M4VO"
   },
   "source": [
    "Let us select the reviews and summaries whose length falls below or equal to **max_text_len** and **max_summary_len**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yY0tEJP0FxhI"
   },
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tR1uh8xSNUma"
   },
   "source": [
    "Remember to add the **START** and **END** special tokens at the beginning and end of the summary. Here, I have chosen **sostok** and **eostok** as START and END tokens\n",
    "\n",
    "**Note:** Be sure that the chosen special tokens never appear in the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwLUH78CFxhg"
   },
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GlcX4RFOh13"
   },
   "source": [
    "We are getting closer to the model building part. Before that, we need to split our dataset into a training and validation set. We’ll use 90% of the dataset as the training data and evaluate the performance on the remaining 10% (holdout set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RakakKHcFxhl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vq1mqyOHOtIl"
   },
   "source": [
    "#Preparing the Tokenizer\n",
    "\n",
    "A tokenizer builds the vocabulary and converts a word sequence to an integer sequence. Go ahead and build tokenizers for text and summary:\n",
    "\n",
    "#Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRHTgX6hFxhq"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzvLwYL_PDcx"
   },
   "source": [
    "#Rarewords and its Coverage\n",
    "\n",
    "Let us look at the proportion rare words and its total coverage in the entire text\n",
    "\n",
    "Here, I am defining the threshold to be 4 which means word whose count is below 4 is considered as a rare word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8KronV2Fxhx",
    "outputId": "d2eb2f27-fbbc-4e61-9556-3c3ff5e4327b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 79.11954765751211\n",
      "Total Coverage of rare words: 29.24548581255374\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "So-J-5kzQIeO"
   },
   "source": [
    "**Remember**:\n",
    "\n",
    "\n",
    "* **tot_cnt** gives the size of vocabulary (which means every unique words in the text)\n",
    " \n",
    "*   **cnt** gives me the no. of rare words whose count falls below threshold\n",
    "\n",
    "*  **tot_cnt - cnt** gives me the top most common words \n",
    "\n",
    "Let us define the tokenizer with top most common words for reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2giEsF3Fxh3"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCbGMsm4FxiA",
    "outputId": "2d9165f0-e542-4114-91f3-e070d483fce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2477"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQfKP3sqRxi9"
   },
   "source": [
    "#Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRHqyBkBFxiJ"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KInA6O6ZSkJz"
   },
   "source": [
    "#Rarewords and its Coverage\n",
    "\n",
    "Let us look at the proportion rare words and its total coverage in the entire summary\n",
    "\n",
    "Here, I am defining the threshold to be 6 which means word whose count is below 6 is considered as a rare word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yzE5OiRLFxiM",
    "outputId": "7f7a4f89-b088-4847-8172-09e5a2383d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 91.28367670364501\n",
      "Total Coverage of rare words: 31.181619256017505\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PBhzKuRSw_9"
   },
   "source": [
    "Let us define the tokenizer with top most common words for summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fswLvIgFxiR"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqwDUT5oTFmn"
   },
   "source": [
    "Let us check whether word count of start token is equal to length of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pR8IX9FRFxiY",
    "outputId": "b116cdbd-42c4-4ede-9f6d-46284115393e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525, 525)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sostok'],len(y_tr)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVFhFVguTTtw"
   },
   "source": [
    "Here, I am deleting the rows that contain only **START** and **END** tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZ-vW82sFxih"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cx5NISuMFxik"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOtlDcthFxip"
   },
   "source": [
    "# Model building\n",
    "\n",
    "We are finally at the model building part. But before we do that, we need to familiarize ourselves with a few terms which are required prior to building the model.\n",
    "\n",
    "**Return Sequences = True**: When the return sequences parameter is set to True, LSTM produces the hidden state and cell state for every timestep\n",
    "\n",
    "**Return State = True**: When return state = True, LSTM produces the hidden state and cell state of the last timestep only\n",
    "\n",
    "**Initial State**: This is used to initialize the internal states of the LSTM for the first timestep\n",
    "\n",
    "**Stacked LSTM**: Stacked LSTM has multiple layers of LSTM stacked on top of each other. \n",
    "This leads to a better representation of the sequence. I encourage you to experiment with the multiple layers of the LSTM stacked on top of each other (it’s a great way to learn this)\n",
    "\n",
    "Here, we are building a 3 stacked LSTM for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXef38nBFxir",
    "outputId": "7ae99521-46f8-4c6f-9cba-4979deffeee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 100)      247700      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 30, 300), (N 481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 30, 300), (N 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    63200       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 30, 300), (N 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 632)    379832      concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,275,832\n",
      "Trainable params: 3,275,832\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZVlfRuMUcoP"
   },
   "source": [
    "I am using sparse categorical cross-entropy as the loss function since it converts the integer sequence to a one-hot vector on the fly. This overcomes any memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lwfi1Fm8Fxiz"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0ykDbxfUhyw"
   },
   "source": [
    "Remember the concept of early stopping? It is used to stop training the neural network at the right time by monitoring a user-specified metric. Here, I am monitoring the validation loss (val_loss). Our model will stop training once the validation loss increases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-A3J92MUljB"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mw6CVECaUq5b"
   },
   "source": [
    "We’ll train the model on a batch size of 128 and validate it on the holdout set (which is 10% of our dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETnPzA4OFxi3",
    "outputId": "477e374f-7cf2-4d60-f86e-2c49c9cebedb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 525 samples, validate on 56 samples\n",
      "Epoch 1/50\n",
      "525/525 [==============================] - 11s 20ms/sample - loss: 5.3027 - val_loss: 3.3022\n",
      "Epoch 2/50\n",
      "525/525 [==============================] - 3s 6ms/sample - loss: 3.5620 - val_loss: 2.7527\n",
      "Epoch 3/50\n",
      "525/525 [==============================] - 3s 7ms/sample - loss: 3.2482 - val_loss: 2.6403\n",
      "Epoch 4/50\n",
      "525/525 [==============================] - 3s 7ms/sample - loss: 3.1859 - val_loss: 2.9134\n",
      "Epoch 5/50\n",
      "525/525 [==============================] - 3s 6ms/sample - loss: 3.3255 - val_loss: 2.6837\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ezKYOp2UxG5"
   },
   "source": [
    "#Understanding the Diagnostic plot\n",
    "\n",
    "Now, we will plot a few diagnostic plots to understand the behavior of the model over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDTNLAURFxjE",
    "outputId": "e2ea6e44-3931-4014-97a1-03fa2a441228"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSyx-HvpUz2o"
   },
   "source": [
    "From the plot, we can infer that validation loss has increased after epoch 17 for 2 successive epochs. Hence, training is stopped at epoch 19.\n",
    "\n",
    "Next, let’s build the dictionary to convert the index to word for target and source vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBX0zZnOFxjW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'sostok', 2: 'eostok', 3: 'great', 4: 'the', 5: 'best', 6: 'good', 7: 'chips', 8: 'not', 9: 'it', 10: 'for', 11: 'these', 12: 'and', 13: 'tea', 14: 'tasty', 15: 'love', 16: 'delicious', 17: 'is', 18: 'taste', 19: 'of', 20: 'this', 21: 'my', 22: 'ever', 23: 'flavor', 24: 'food', 25: 'very', 26: 'excellent', 27: 'are', 28: 'favorite', 29: 'but', 30: 'product', 31: 'chip', 32: 'in', 33: 'stuff', 34: 'coffee', 35: 'hot', 36: 'as', 37: 'you', 38: 'no', 39: 'salt', 40: 'awesome', 41: 'yummy', 42: 'sauce', 43: 'chocolate', 44: 'buy', 45: 'price', 46: 'healthy', 47: 'better', 48: 'to', 49: 'fresh', 50: 'vinegar', 51: 'with', 52: 'dog', 53: 'tasting', 54: 'kettle', 55: 'sugar', 56: 'what', 57: 'wonderful', 58: 'twizzlers', 59: 'loves', 60: 'yum', 61: 'make', 62: 'oil', 63: 'potato', 64: 'free', 65: 'baby', 66: 'item', 67: 'too', 68: 'snack', 69: 'be', 70: 'amazing', 71: 'deal', 72: 'an', 73: 'tasted', 74: 'mix', 75: 'they', 76: 'energy', 77: 'our', 78: 'at', 79: 'tortilla', 80: 'quality', 81: 'tastes', 82: 'spicy', 83: 'perfect', 84: 'mm', 85: 'much', 86: 'awful', 87: 'on', 88: 'oatmeal', 89: 'loved', 90: 'never', 91: 'all', 92: 'around', 93: 'low', 94: 'crunchy', 95: 'dinner', 96: 'your', 97: 'by', 98: 'plocky', 99: 'happy', 100: 'has', 101: 'candy', 102: 'banana', 103: 'runny', 104: 'really', 105: 'gravy', 106: 'bad', 107: 'could', 108: 'them', 109: 'pretty', 110: 'son', 111: 'gum', 112: 'altoids', 113: 'cannot', 114: 'find', 115: 'that', 116: 'gluten', 117: 'will', 118: 'tangy', 119: 'go', 120: 'breakfast', 121: 'nice', 122: 'tartlets', 123: 'expected', 124: 'only', 125: 'looks', 126: 'beans', 127: 'so', 128: 'roast', 129: 'sell', 130: 'cat', 131: 'just', 132: 'salsa', 133: 'egg', 134: 'allergy', 135: 'mints', 136: 'me', 137: 'little', 138: 'ham', 139: 'base', 140: 'cup', 141: 'arrived', 142: 'fast', 143: 'disappointing', 144: 'one', 145: 'pocky', 146: 'sticks', 147: 'burns', 148: 'ok', 149: 'off', 150: 'poor', 151: 'runts', 152: 'greatest', 153: 'basket', 154: 'absolutely', 155: 'dont', 156: 'get', 157: 'natural', 158: 'juice', 159: 'from', 160: 'down', 161: 'how', 162: 'sweet', 163: 'anywhere', 164: 'oz', 165: 'grain', 166: 'caramel', 167: 'out', 168: 'have', 169: 'simple', 170: 'who', 171: 'though', 172: 'oats', 173: 'cups', 174: 'fantastic', 175: 'likes', 176: 'sure', 177: 'creamy', 178: 'smalls', 179: 'allergic', 180: 'expensive', 181: 'fruit', 182: 'taffy', 183: 'bloody', 184: 'mary', 185: 'different', 186: 'bar', 187: 'convenient', 188: 'raw', 189: 'than', 190: 'simply', 191: 'rice', 192: 'cats', 193: 'agave', 194: 'mate', 195: 'creamer', 196: 'steam', 197: 'berry', 198: 'stale', 199: 'real', 200: 'addictive', 201: 'do', 202: 'does', 203: 'cookies', 204: 'wrong', 205: 'delivered', 206: 'had', 207: 'everyday', 208: 'bite', 209: 'disappointed', 210: 'surprise', 211: 'was', 212: 'bags', 213: 'way', 214: 'strawberry', 215: 'highly', 216: 'hard', 217: 'organic', 218: 'half', 219: 'kona', 220: 'tassimo', 221: 'exactly', 222: 'ice', 223: 'burnt', 224: 'huge', 225: 'fan', 226: 'mushy', 227: 'teething', 228: 'easy', 229: 'black', 230: 'gods', 231: 'english', 232: 'tasten', 233: 'holistic', 234: 'select', 235: 'lots', 236: 'expect', 237: 'heinz', 238: 'more', 239: 'sodium', 240: 'slight', 241: 'jalapeno', 242: 'thick', 243: 'sons', 244: 'ferret', 245: 'weak', 246: 'knees', 247: 'even', 248: 'seems', 249: 'overpriced', 250: 'msg', 251: 'joe', 252: 'health', 253: 'toasted', 254: 'sesame', 255: 'far', 256: 'pretzel', 257: 'haven', 258: 'aluminum', 259: 'favorites', 260: 'timely', 261: 'delivery', 262: 'miss', 263: 'vickie', 264: 'happened', 265: 'recipe', 266: 'changed', 267: 'rip', 268: 'packaging', 269: 'nothing', 270: 'special', 271: 'heads', 272: 'defacto', 273: 'standard', 274: 'value', 275: 'review', 276: 'kind', 277: 'bland', 278: 'small', 279: 'worked', 280: 'quick', 281: 'arrival', 282: 'case', 283: 'white', 284: 'know', 285: 'if', 286: 'tao', 287: 'don', 288: 'music', 289: 'palate', 290: 'thin', 291: 'diet', 292: 'ingredients', 293: 'realemon', 294: 'amazon', 295: 'please', 296: 'mexico', 297: 'here', 298: 'bbq', 299: 'gone', 300: 'hill', 301: 'allday', 302: 'work', 303: 'extra', 304: 'absotively', 305: 'posilutely', 306: 'success', 307: 'boost', 308: 'handy', 309: 'carb', 310: 'angel', 311: 'puffs', 312: 'eating', 313: 'years', 314: 'some', 315: 'weigh', 316: 'fluid', 317: 'ounces', 318: 'three', 319: 'always', 320: 'together', 321: 'unsalted', 322: 'non', 323: 'gmo', 324: 'getting', 325: 'nd', 326: 'dish', 327: 'pot', 328: 'garbage', 329: 'sad', 330: 'outcome', 331: 'jell', 332: 'nasty', 333: 'keeps', 334: 'dentest', 335: 'chair', 336: 'kept', 337: 'secret', 338: 'substitute', 339: 'soy', 340: 'furniture', 341: 'polish', 342: 'wise', 343: 'star', 344: 'shipped', 345: 'people', 346: 'run', 347: 'yay', 348: 'barley', 349: 'quite', 350: 'greasy', 351: 'addicted', 352: 'supreme', 353: 'feeder', 354: 'liquor', 355: 'serious', 356: 'bucks', 357: 'awsome', 358: 'kids', 359: 'neighborhood', 360: 'us', 361: 'madhouse', 362: 'munchies', 363: 'wintergreen', 364: 'pok', 365: 'chops', 366: 'kitties', 367: 'brands', 368: 'kleri', 369: 'works', 370: 'scottie', 371: 'eat', 372: 'mcclures', 373: 'looking', 374: 'barbeque', 375: 'perfection', 376: 'wafers', 377: 'odd', 378: 'broken', 379: 'most', 380: 'loose', 381: 'leaf', 382: 'rooibos', 383: 'wild', 384: 'chick', 385: 'brown', 386: 'syrup', 387: 'skin', 388: 'lips', 389: 'penguin', 390: 'pooper', 391: 'garbonzo', 392: 'bean', 393: 'flour', 394: 'heaven', 395: 'city', 396: 'brew', 397: 'omaha', 398: 'apple', 399: 'yoli', 400: 'moms', 401: 'beware', 402: 'plastic', 403: 'wary', 404: 'lilly', 405: 'says', 406: 'thumbs', 407: 'up', 408: 'marinade', 409: 'bit', 410: 'side', 411: 'needs', 412: 'when', 413: 'smokin', 414: 'yes', 415: 'big', 416: 'tub', 417: 'ahmad', 418: 'franch', 419: 'cough', 420: 'medicine', 421: 'else', 422: 'geat', 423: 'omg', 424: 'jelly', 425: 'belly', 426: 'delisious', 427: 'pancakes', 428: 'specialty', 429: 'party', 430: 'paid', 431: 'break', 432: 'easily', 433: 'eco', 434: 'smokey', 435: 'chipotle', 436: 'whole', 437: 'tortill', 438: 'baking', 439: 'soda', 440: 'bomb', 441: 'unique', 442: 'schrumshist', 443: 'correct', 444: 'double', 445: 'pleasure', 446: 'kiss', 447: 'bars', 448: 'diets', 449: 'mediocre', 450: 'cheap', 451: 'hey', 452: 'nearly', 453: 'killed', 454: 'like', 455: 'licorice', 456: 'preventing', 457: 'cramps', 458: 'caution', 459: 'support', 460: 'sweetener', 461: 'lovely', 462: 'spice', 463: 'grow', 464: 'bitter', 465: 'firm', 466: 'guests', 467: 'hearty', 468: 'am', 469: 'second', 470: 'bag', 471: 'can', 472: 'service', 473: 'dark', 474: 'day', 475: 'waste', 476: 'money', 477: 'choclate', 478: 'tasteless', 479: 'calorie', 480: 'royal', 481: 'canin', 482: 'cocker', 483: 'time', 484: 'human', 485: 'edible', 486: 'strictly', 487: 'gag', 488: 'gift', 489: 'toss', 490: 'jars', 491: 'porcini', 492: 'mushrooms', 493: 'fully', 494: 'loaded', 495: 'baked', 496: 'asparagus', 497: 'bliss', 498: 'olive', 499: 'indulgence', 500: 'reusable', 501: 'containers', 502: 'fat', 503: 'dairy', 504: 'impressed', 505: 'target', 506: 'where', 507: 'been', 508: 'last', 509: 'cuttiest', 510: 'century', 511: 'kikkoman', 512: 'dripping', 513: 'home', 514: 'twizlers', 515: 'fabulous', 516: 'french', 517: 'since', 518: 'slice', 519: 'bread', 520: 'order', 521: 'cheaper', 522: 'locally', 523: 'falafel', 524: 'sweetner', 525: 'outstanding', 526: 'wife', 527: 'lemon', 528: 'store', 529: 'adorable', 530: 'makes', 531: 'presentation', 532: 'sour', 533: 'should', 534: 'mixer', 535: 'near', 536: 'glad', 537: 'size', 538: 'traditional', 539: 'lovers', 540: 'gaaak', 541: 'extreme', 542: 'disgusting', 543: 'issues', 544: 'additive', 545: 'speedy', 546: 'shipping', 547: 'or', 548: 'chivey', 549: 'recomended', 550: 'goodness', 551: 'smaller', 552: 'nantucket', 553: 'blend', 554: 'sugarfree', 555: 'addicitive', 556: 'ramen', 557: 'terrible', 558: 'bavarian', 559: 'creme', 560: 'delish', 561: 'dissapointed', 562: 'same', 563: 'wow', 564: 'hazelnut', 565: 'sensitive', 566: 'dogs', 567: 'heavy', 568: 'fanfreakintastic', 569: 'earth', 570: 'lentil', 571: 'horrible', 572: 'pricey', 573: 'new', 574: 'granola', 575: 'tart', 576: 'oyster', 577: 'source', 578: 'electrolytes', 579: 'shrimp', 580: 'stir', 581: 'fry', 582: 'child', 583: 'nectar', 584: 'king', 585: 'seasoning', 586: 'salts', 587: 'famous', 588: 'reason', 589: 'pioneer', 590: 'chicken', 591: 'noodle', 592: 'soup', 593: 'use', 594: 'once', 595: 'em', 596: 'beautiful', 597: 'such', 598: 'ehhh', 599: 'drain', 600: 'instant', 601: 'balance', 602: 'crunchiness', 603: 'moisture', 604: 'god', 605: 'laxative', 606: 'consistently', 607: 'stomach', 608: 'problems', 609: 'convenience', 610: 'quantity', 611: 'disappointment', 612: 'ones', 613: 'kcups', 614: 'box', 615: 'anytime', 616: 'tested', 617: 'trucker', 618: 'sea', 619: 'iodine', 620: 'dad', 621: 'liked', 622: 'worst', 623: 'frosting', 624: 'marley', 625: 'mellow', 626: 'mood', 627: 'lite', 628: 'lemonade', 629: 'freah', 630: 'bright', 631: 'clean'}\n"
     ]
    }
   ],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index\n",
    "print(reverse_target_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eM_nU_VvFxjq"
   },
   "source": [
    "# Inference\n",
    "\n",
    "Set up the inference for the encoder and decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QkrNV-4Fxjt"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOiyk4ToWe74"
   },
   "source": [
    "We are defining a function below which is the implementation of the inference process (which we covered [here](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6f6TTFnBFxj6"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GuDf4TPWt6_"
   },
   "source": [
    "Let us define the functions to convert an integer sequence to a word sequence for summary as well as the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAUntznIFxj9"
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gM4ALyfWwA9"
   },
   "source": [
    "Here are a few summaries generated by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUtQmQTmFxkI",
    "outputId": "f407d9fc-e0cd-4082-98f5-bd1f562dc26f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: coffee available tassimo kona richest flavor fantastic aroma byfar favorite \n",
      "Original summary: kona for tassimo \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: nice alternative apple pie love fact slicing dicing easy prepare also loved fact make fresh whenever needed \n",
      "Original summary: loved these tartlets \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: product exactly advertised fresh unfortunately keep candy dish office going fast need reorder keep demand \n",
      "Original summary: excellent exactly what expected \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: product impossible wash actually get cleaned could get food pieces save life tried scrubbing even washed dishwasher suggests \n",
      "Original summary: only good for ice \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: bags lot overcooked brown pieces also felt greasy keep wiping fingers napkin \n",
      "Original summary: burnt \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: love chips unique taste incredible crispy texture used get henry san diego ca moved ordering amazon since running constantly \n",
      "Original summary: huge fan of these chips \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: flavors good however see differce oaker oats brand mushy \n",
      "Original summary: mushy \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: loved product fruits food want new baby choke also used put ice baby teething worked great well food like bananas though impossible get clean \n",
      "Original summary: great for teething \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: easy looks came enough yellow frosting frosting stick well year old like making though \n",
      "Original summary: not as easy as it looks \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: fantastic chips ever could eat whole bag made lots whole grain beans make complete protein qualms feeding kids snacks great problem bag small need bigger bag \n",
      "Original summary: black beans never tasted so good \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: best way cook roast local grocery went buisness could find product till looked please continue keep product available \n",
      "Original summary: best roast ever \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: appreciate fact buy product online free shipping great around mix good wonderful texture thanks amazon \n",
      "Original summary: great all around mix \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: numi winner tea husband son often share wonderful tea dinner pleasant reason love turn bitter longer steeping like teas \n",
      "Original summary: wonderful tea \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: flavor lasted longer would probably die starvation lack wanting take mouth \n",
      "Original summary: the flavor of the gods \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: bought husband currently overseas loves apparently staff likes also generous amounts twizzlers ounce bag well worth price twizzlers strawberry ounce bags \n",
      "Original summary: twizzlers \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: true seattle coffee addict never better coffee candy \n",
      "Original summary: the best \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: fresh scent taste easy eat one tin hours use favors everyone loved \n",
      "Original summary: fresh \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: nuts arrived fresh tasty broken dirty looking like cashews sometimes \n",
      "Original summary: fresh and tasty \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: better england know maybe nostalgia part \n",
      "Original summary: not as good as the english sell \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: ordered line received days tea excellent price right \n",
      "Original summary: good tasten tea \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: cats thrive extremely well dry cat food definitely much less hair ball throw ups fur great fit weight vendor ships extremely fast one top amazon suppliers book \n",
      "Original summary: holistic select cat food \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: daughter loves twizzlers shipment six pounds really hit spot exactly would expect six packages strawberry twizzlers \n",
      "Original summary: lots of twizzlers just what you expect \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: chips perfect snacking without salsa texture brittle thin like plocky chips downside spice minimal \n",
      "Original summary: tasty \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: love earth best organic baby food tastiest one tried think pretty gross daughter still eats much enthusiasm flavors got decent amount protein important us vegetarian family \n",
      "Original summary: good for you but not best flavor \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: ridiculous best gourmet ketchup ever tasted pricey wow love stuff would make great gift someone hard buy loves ketchup yummy \n",
      "Original summary: heinz no more \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: tried lot chips best real sea salt real vinegar mouth watering flavor crispiness faint heart flavor authentic \n",
      "Original summary: best salt vinegar \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: mg sodium per serving chips crisp sometimes dip low salt ketchup adds bit flavor \n",
      "Original summary: great chips with very low sodium \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: one best chips buy tried kettle varieties many others slight taste jalapeno delicious perfect sandwich alone like addicted chip see addicts anonymous \n",
      "Original summary: slight taste of jalapeno \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: chips thick crunchy absolutely love taste perfect like buy smaller bags cant trust large bag taste good \n",
      "Original summary: these chips they are thick and crunchy \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: son favorite dinner best seems best combo protein vitamin runnier dinners great start solids protein love love love brand flavor think purchased jars far \n",
      "Original summary: sons favorite dinner \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: bought husband said best energy shots takes one mornings works hard day good stuff \n",
      "Original summary: great energy \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: ferret loves rabbit nice high protein treat addition regular evo ferret diet price reasonable compared local pet store often item stock \n",
      "Original summary: our ferret loves this \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: always try food feed daughter one actually pretty good daughter loves gets excited make \n",
      "Original summary: yum \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: great dog food dog severs allergies brand one feed \n",
      "Original summary: great dog food \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: one best salsas found long time stay away variety pack two come worth money \n",
      "Original summary: love the salsa \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: biscuits delicious sweet enough crunch love quick delivery would order satisfied \n",
      "Original summary: delicious \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: glad found mix make pancakes without eggs son allergy taste fine family like many ways \n",
      "Original summary: good for egg allergy \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: huge supply still working plenty spare much effective buying one grocery store every time want mint \n",
      "Original summary: these mints are awesome \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: chips remind long obsolete brand chips could get middle school far tastiest chips impossible walk past aisle without picking trademark blue bag putting bag clip bag \n",
      "Original summary: chips make me weak at the knees \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: looks pretty contents minute seem high quality plastic cup tea looks like came dollar store basket nice though \n",
      "Original summary: even at seems little overpriced \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: great couldnt stop eating lowest priced anywhere buy even next time cant say enough shipped faster paid shipped also thank much \n",
      "Original summary: great buy \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: bought daughter grad party well strawberry loves things great product \n",
      "Original summary: great product \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: delicious chocolate excellent espresso bean perfect roast purpose crunchy bitter chocolate covered espresso beans pounds \n",
      "Original summary: good stuff \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: used ham base loaded msg realize ordered return food item \n",
      "Original summary: msg ham base \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: daughter outgrowing baby food still eat tried rd foods like texture really good likes keep buying \n",
      "Original summary: favorite \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: bold blend great taste flavor comes bursting usually brew drink organic sumatra mandeling bj use blend exclusively get cup rivals complex flavor tassimo brewer fantastic come amazon add subscription service \n",
      "Original summary: good tasting cup joe \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: amazed quickly arrived cute stocking stuffer ok far user appeal get pay \n",
      "Original summary: arrived fast \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: expecting terms company reputation excellent home delivery products \n",
      "Original summary: disappointing \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: product great job clearing kidneys helped make husband feel lots better even though early renal failure \n",
      "Original summary: for your health \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: love candy weight watchers cut back still craving \n",
      "Original summary: twizzlers \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: order arrived advertised chances use awesome flavor exactly hoped may take finish entire gallon definnantly order \n",
      "Original summary: toasted sesame oil \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: chips quite tasty price right packaged well would buy \n",
      "Original summary: very good chips at great price \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: salt free product purchased chips quite greasy \n",
      "Original summary: potato chips \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: green mountain favorite brand nantucket favorite coffee green mountain smooth flavor strong weak \n",
      "Original summary: my favorite \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: far favorite chips extremely crunchy extreme vinegar flavor differentiates lesser fried potato snacks tried several brands salt vinegar none come close \n",
      "Original summary: by far my favorite chips \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: sooooo deliscious bad ate em fast gained pds fault \n",
      "Original summary: pretzel haven \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: baking soda works great cooking health reasons take acid indigestion heart burn tsp half glass water gone use make body alkaline great changing ph level body \n",
      "Original summary: aluminum free \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: one son favorite baby foods bit sweetness sweet runny flavors use deeper spoon one add little rice cereal thicken \n",
      "Original summary: one of our favorites \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: tea best tea ever first tasted norway find states every morning \n",
      "Original summary: delicious \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: good job shipping order quickly nothing like ace hardware took week ship \n",
      "Original summary: very timely delivery \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: chips ones found tasty healthy fewer fat calories plus higher fiber want good taste nutrition perfect blend \n",
      "Original summary: plocky tortilla chips tasty and healthy \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: many purchses made amazon past december pleased pocky chocolate cream covered biscuit sticks arrived time hit surprised gift sure keep refridgerated \n",
      "Original summary: very happy with pocky sticks \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: peanut lover much larger cocktail peanuts six people christmas gift list ask every year \n",
      "Original summary: the best \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: daughter loved stuff natural made well good mix things get older beats gerber \n",
      "Original summary: they make the best baby food \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: love spicy ramen whatever reasons thing burns stomach badly burning sensation go away like hours sure healthy buy walmart way cheaper amazon \n",
      "Original summary: it burns \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: miss vickies better unfortunately less widely available sure tough right word noticeable difference two brands miss vickie better imho \n",
      "Original summary: ok but miss vickie are better \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: like order kettle spicy thai chips amazon hard find locally probably aggrieved know recipe changed like regular potatoe chips paid husband gets eat like spicy thai fine yuck \n",
      "Original summary: what happened the recipe has changed \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: singles sell store box singles sure amazon selling box singles hazelnut coffee creamer favorite truly good buy \n",
      "Original summary: rip off price \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: soft smooth texture little one devours one one best meat veg dinners found \n",
      "Original summary: favorite \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: got kids christmas stocking stuffers love far delivery product came good condition timely terrific definitely good buy \n",
      "Original summary: these are great \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: found crisps local walmart figured would give try yummy may never go back regular chips big chip fan anyway problem eat entire bag one sitting give crisps big thumbs \n",
      "Original summary: yummy \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: love tasty infact think addicted buying packs bags reasonable going target getting bag savings bag use subscribe save several product love subscribe save \n",
      "Original summary: love these \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: mix poorly packaged breaks open easily shipping nearly full bag order spilled shipping box bags come unsealed top tears packages \n",
      "Original summary: poor item packaging \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: bought thi dad surprise birthday present could find else \n",
      "Original summary: candy \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: salt vinegar chips definitely favorite type potato chip ones made kettle tasty enjoyed thoroughly wish healthy option great taste \n",
      "Original summary: very good \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: okay would go way buy \n",
      "Original summary: nothing special \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: best chips ever eaten everyone family loves sweet spicy crispy really good delivery perfect crushed chips fantastic overall \n",
      "Original summary: yummy chips \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: item fine banana heads like banana runts alot smaller also received item packaging great banana heads come open shipping packing envelope \n",
      "Original summary: banana heads not banana runts \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: found product kettle chips unsalted fabulous found accident amazon hunting local stores crisp tasty without salt \n",
      "Original summary: kettle chips \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: always liked salt vinegar chips time stronger flavor bottom bag vinegar overflowing right first chip doesnt stop finished bag addictive like salt vinegar ultimate chips nothing else compares \n",
      "Original summary: defacto standard for salt and vinegar chips \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better \n",
      "Original summary: good quality dog food \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: despite coming extremely large box found great value bags preserved reasonable expiration date months away would definitely recommend \n",
      "Original summary: great value \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: love love green tea hard find area places internet charge big price usually get many boxes merchant definitely order seller thanks depend green tea fix everyday \n",
      "Original summary: tea review \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: flavorful enough taste seems seasoning falls chips sinks bottom bag bad really liked idea healthier potato chip would buy \n",
      "Original summary: kind of bland \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: always purchased star kist tuna thought would try brand change pace taste tuna pleasant much basil spices \n",
      "Original summary: not the greatest tasting \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: tasty convenient bars people celiac disease seem gotten smaller time taste convenience outweigh reduction size \n",
      "Original summary: small but good \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: purchased giveaway baby shower jungle theme worked perfect small bags candy purchased want small bags would purchase larger bag \n",
      "Original summary: worked great \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: popchips good following weight watchers lost pounds goal popchips salty crunchy indulgence taste good \n",
      "Original summary: love love love \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: baby loves sweet potatoes essentially mix food sweet potatoes eat case could good bad thing depending wanted mix horribly runny like spooning formula mouth ick \n",
      "Original summary: too runny \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: great tasting tea looking good sassafras tea forever finally ordering product thanks kim \n",
      "Original summary: great product \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: purchased gift mother lightning deal happy loved basket enjoyed cookies flavors tea included shipped quick arrived expected \n",
      "Original summary: quick arrival great basket \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: stuff great low glycemic substitute sugar body great favor size economical shipping fast got mine soon \n",
      "Original summary: healthy stuff \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: arrowhead mills buttermilk pancake mix favorite mix tastes great cooks light fluffy super easy make although made using water think better made milk love pure organic maple syrup \n",
      "Original summary: great mix \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: delicious soy sauce ever tasted surely order deep flavor \n",
      "Original summary: great sauce \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: mccann oatmeal every morning ordering amazon able save almost per box great product tastes great healthy \n",
      "Original summary: food great \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: individually wrapped pieces delicious mildly sweet sugar free aftertaste even traditional marzipan aholic enjoy \n",
      "Original summary: absolutely delicious \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: great price excellent chip cents ounce bag bargain well tasty chip everyone let try impressed commented good taste yet find kettle chip disappointed \n",
      "Original summary: tasty \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: good tasting light chip value nutrition everyone get munchies good alternative eating potato chips \n",
      "Original summary: great healthy snack \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: tomatoes star classic flavor clean really fresh perfect avocado chicken sandwich spicy flavor complex best ketchup ever tried good used foods would make seem crazy \n",
      "Original summary: this is really good stuff \n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: buy order case cannot even remember first time tasted anchovy stuffed olive tried several brands online brand favorite \n",
      "Original summary: best by the case \n",
      "Predicted summary:  great\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,100):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTkaYNjHW4lC"
   },
   "source": [
    "This is really cool stuff. Even though the actual summary and the summary generated by our model do not match in terms of words, both of them are conveying the same meaning. Our model is able to generate a legible summary based on the context present in the text.\n",
    "\n",
    "This is how we can perform text summarization using deep learning concepts in Python.\n",
    "\n",
    "#How can we Improve the Model’s Performance Even Further?\n",
    "\n",
    "Your learning doesn’t stop here! There’s a lot more you can do to play around and experiment with the model:\n",
    "\n",
    "I recommend you to **increase the training dataset** size and build the model. The generalization capability of a deep learning model enhances with an increase in the training dataset size\n",
    "\n",
    "Try implementing **Bi-Directional LSTM** which is capable of capturing the context from both the directions and results in a better context vector\n",
    "\n",
    "Use the **beam search strategy** for decoding the test sequence instead of using the greedy approach (argmax)\n",
    "\n",
    "Evaluate the performance of your model based on the **BLEU score**\n",
    "\n",
    "Implement **pointer-generator networks** and **coverage mechanisms**\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_qIecuvY5GT"
   },
   "source": [
    "#End Notes\n",
    "\n",
    "If you have any feedback on this article or any doubts/queries, kindly share them in the comments section over [here](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/) and I will get back to you. And make sure you experiment with the model we built here and share your results with me!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "How to build own text summarizer using deep learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
